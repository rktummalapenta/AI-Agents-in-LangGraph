{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import aiosqlite\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c033522-d2fc-41ac-8e3c-5e35872bf88d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bd777b6-86aa-453e-a67a-0df81d8ca2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        #graph.add_node(\"action\", self.take_action)\n",
    "        #graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        #graph.add_edge(\"action\", \"llm\")\n",
    "        graph.add_edge(\"llm\", END)\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        #self.model = model.bind_tools(tools)\n",
    "        self.model = model\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        print(f\"State Message: {state}\")\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        print(f\"Messages: {messages}\")\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot1 = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "663dc72f-58df-44f3-9cb5-fa42a372e72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Message: {'messages': [HumanMessage(content='What is the weather in LA?', additional_kwargs={}, response_metadata={}), AIMessage(content='To provide you with the current weather in Los Angeles, I need to look up the latest information. Let me do that for you. Please hold on for a moment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 83, 'total_tokens': 117, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-f36f34f5-7ce8-4707-b9c3-180924544d5d-0', usage_metadata={'input_tokens': 83, 'output_tokens': 34, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is the weather in NYC?', additional_kwargs={}, response_metadata={}), AIMessage(content='Let me look up the current weather conditions in New York City for you. Please hold on for a moment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 132, 'total_tokens': 154, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-a53781a1-9056-4b74-98d9-a22c34b8241d-0', usage_metadata={'input_tokens': 132, 'output_tokens': 22, 'total_tokens': 154, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is the weather in NJ?', additional_kwargs={}, response_metadata={})]}\n",
      "Messages: [SystemMessage(content='You are a smart research assistant. Use the search engine to look up information. You are allowed to make multiple calls (either together or in sequence). Only look up information when you are sure of what you want. If you need to look up some information before asking a follow up question, you are allowed to do that!\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the weather in LA?', additional_kwargs={}, response_metadata={}), AIMessage(content='To provide you with the current weather in Los Angeles, I need to look up the latest information. Let me do that for you. Please hold on for a moment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 83, 'total_tokens': 117, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-f36f34f5-7ce8-4707-b9c3-180924544d5d-0', usage_metadata={'input_tokens': 83, 'output_tokens': 34, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is the weather in NYC?', additional_kwargs={}, response_metadata={}), AIMessage(content='Let me look up the current weather conditions in New York City for you. Please hold on for a moment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 132, 'total_tokens': 154, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-a53781a1-9056-4b74-98d9-a22c34b8241d-0', usage_metadata={'input_tokens': 132, 'output_tokens': 22, 'total_tokens': 154, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What is the weather in NJ?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in NJ?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "response = abot1.graph.invoke({'messages':messages},thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "268d6ce7-697d-4d8e-a1bc-8c92058840b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9KxudblNCqa97EP2OB7lBm30', 'function': {'arguments': '{\"query\":\"current weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 151, 'total_tokens': 173, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ff73d830-8000-4552-9312-67d2dcdd41b4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_9KxudblNCqa97EP2OB7lBm30', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 22, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_9KxudblNCqa97EP2OB7lBm30', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1731196275, \\'localtime\\': \\'2024-11-09 15:51\\'}, \\'current\\': {\\'last_updated_epoch\\': 1731195900, \\'last_updated\\': \\'2024-11-09 15:45\\', \\'temp_c\\': 18.3, \\'temp_f\\': 64.9, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 5.4, \\'wind_kph\\': 8.6, \\'wind_degree\\': 260, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1015.0, \\'pressure_in\\': 29.96, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 58, \\'cloud\\': 50, \\'feelslike_c\\': 18.3, \\'feelslike_f\\': 64.9, \\'windchill_c\\': 16.6, \\'windchill_f\\': 61.8, \\'heatindex_c\\': 16.6, \\'heatindex_f\\': 61.8, \\'dewpoint_c\\': 9.4, \\'dewpoint_f\\': 49.0, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 1.0, \\'gust_mph\\': 7.7, \\'gust_kph\\': 12.3}}\"}, {\\'url\\': \\'https://forecast.weather.gov/MapClick.php?lat=37.77493&lon=-122.419418\\', \\'content\\': \\'Current conditions at SAN FRANCISCO DOWNTOWN (SFOC1) Lat: 37.77056°NLon: 122.42694°WElev: 150.0ft. NA. 52°F. 11°C. ... 2024-6pm PST Nov 9, 2024 . Forecast Discussion . Additional Resources. ... National Weather Service; San Francisco Bay Area, CA; 21 Grace Hopper Ave, Stop 5; Monterey, CA 93943-5505; Comments? Questions?\\'}]', name='tavily_search_results_json', tool_call_id='call_9KxudblNCqa97EP2OB7lBm30')]\n",
      "[AIMessage(content='The current weather in San Francisco is partly cloudy with a temperature of approximately 18.3°C (64.9°F). The wind is blowing from the west at 5.4 mph (8.6 kph), and the humidity is at 58%. There is no precipitation at the moment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 734, 'total_tokens': 796, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-d1bb41d7-96e5-40a2-840e-1d0a286e64d2-0', usage_metadata={'input_tokens': 734, 'output_tokens': 62, 'total_tokens': 796, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Could you please provide more context or specify what information you\\'re looking for regarding \"la\"? Are you asking about Los Angeles, a specific event, or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 149, 'total_tokens': 182, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-db6ddc12-0f50-4821-8bdc-dca253cc3753-0', usage_metadata={'input_tokens': 149, 'output_tokens': 33, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"What about in la?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content=\"Could you please provide more context or specify what you're comparing in terms of warmth? For example, are you asking about the weather in two different locations, the temperature of two different objects, or something else entirely?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 149, 'total_tokens': 192, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e0e2f295-e009-433d-99f4-e888cf73f689-0', usage_metadata={'input_tokens': 149, 'output_tokens': 43, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Could you please specify the two items, locations, or time periods you would like to compare in terms of warmth?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 149, 'total_tokens': 173, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8ff0eca-fdaa-4a74-9f06-74d28ac794ff-0', usage_metadata={'input_tokens': 149, 'output_tokens': 24, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "    for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace59a36-3941-459e-b9d1-ac5a4a1ed3ae",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_klzux1F9PLMH9MQ6kQ81uYxN', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| clear| with| a| temperature| of| |14|.|3|°C| (|57|.|7|°F|).| The| wind| is| blowing| from| the| west| at| |6|.|5| mph| (|10|.|4| k|ph|),| and| the| humidity| is| at| |41|%.| There| is| no| precipitation|,| and| the| visibility| is| |13| km| (|8| miles|).|"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of OpenAI means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55865e2f-a9cc-421d-9f80-033b3557bb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
